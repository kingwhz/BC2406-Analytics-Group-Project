geom_text(
aes(label=signif(..count.. / tapply(..count.., ..x.., sum)[as.character(..x..)], digits=3)),
stat="count",
position=position_fill(vjust=0.5), size = 4)
#Target vs Number of Major Blood Vessels
d6 = ggplot(data = heart.dt, aes(x = ca, fill = target)) + geom_bar(position = "fill") +
labs(x = "Number of Major Blood Vessels", y = "Proportion", title = "Target vs Number of Major Blood Vessels") +
geom_text(
aes(label=signif(..count.. / tapply(..count.., ..x.., sum)[as.character(..x..)], digits=3)),
stat="count",
position=position_fill(vjust=0.5), size = 4)
#Target vs Gradient of Slope
d7 = ggplot(data = heart.dt, aes(x = slope, fill = target)) + geom_bar(position = "fill") +
labs(x = "Gradient of Slope", y = "Proportion", title = "Target vs Gradient of Slope") +
geom_text(
aes(label=signif(..count.. / tapply(..count.., ..x.., sum)[as.character(..x..)], digits=3)),
stat="count",
position=position_fill(vjust=0.5), size = 4)
w <- ggarrange(d1, d2 ,d3, d4, d5, d6, d7 + rremove("x.text"),ncol = 2, nrow = 4)
annotate_figure(x,top = text_grob("Relationship between categorical X variables", color = "black", face = "bold", size = 14))
#categorical vs continuous
cc.1.1 <-ggplot(heart.dt, aes(x=age, y=sex)) + geom_violin()
cc.1.2 <-ggplot(heart.dt, aes(x=age, y=cp)) + geom_violin()
cc.1.3 <-ggplot(heart.dt, aes(x=age, y=fbs)) + geom_violin()
cc.1.4 <-ggplot(heart.dt, aes(x=age, y=restecg)) + geom_violin()
cc.1.5 <-ggplot(heart.dt, aes(x=age, y=exang)) + geom_violin()
cc.1.6 <-ggplot(heart.dt, aes(x=age, y=ca)) + geom_violin()
cc.1.7 <-ggplot(heart.dt, aes(x=age, y=thal)) + geom_violin()
cc.1.8 <-ggplot(heart.dt, aes(x=age, y=slope)) + geom_violin()
a <- ggarrange(cc.1.1, cc.1.2 ,cc.1.3, cc.1.4, cc.1.5, cc.1.6, cc.1.7, cc.1.8 + rremove("x.text"),ncol = 4, nrow = 2)
annotate_figure(a,top = text_grob("age vs Continuous X variables", color = "black", face = "bold", size = 14))
cc.2.1 <-ggplot(heart.dt, aes(x=trestbps, y=sex)) + geom_violin()
cc.2.2 <-ggplot(heart.dt, aes(x=trestbps, y=cp)) + geom_violin()
cc.2.3 <-ggplot(heart.dt, aes(x=trestbps, y=fbs)) + geom_violin()
cc.2.4 <-ggplot(heart.dt, aes(x=trestbps, y=restecg)) + geom_violin()
cc.2.5 <-ggplot(heart.dt, aes(x=trestbps, y=exang)) + geom_violin()
cc.2.6 <-ggplot(heart.dt, aes(x=trestbps, y=ca)) + geom_violin()
cc.2.7 <-ggplot(heart.dt, aes(x=trestbps, y=thal)) + geom_violin()
cc.2.8 <-ggplot(heart.dt, aes(x=trestbps, y=slope)) + geom_violin()
b <- ggarrange(cc.2.1, cc.2.2 ,cc.2.3, cc.2.4, cc.2.5, cc.2.6, cc.2.7, cc.2.8 + rremove("x.text"),ncol = 4, nrow = 2)
annotate_figure(b,top = text_grob("trestbps vs Continuous X variables", color = "black", face = "bold", size = 14))
cc.3.1 <-ggplot(heart.dt, aes(x=chol, y=sex)) + geom_violin()
cc.3.2 <-ggplot(heart.dt, aes(x=chol, y=cp)) + geom_violin()
cc.3.3 <-ggplot(heart.dt, aes(x=chol, y=fbs)) + geom_violin()
cc.3.4 <-ggplot(heart.dt, aes(x=chol, y=restecg)) + geom_violin()
cc.3.5 <-ggplot(heart.dt, aes(x=chol, y=exang)) + geom_violin()
cc.3.6 <-ggplot(heart.dt, aes(x=chol, y=ca)) + geom_violin()
cc.3.7 <-ggplot(heart.dt, aes(x=chol, y=thal)) + geom_violin()
cc.3.8 <-ggplot(heart.dt, aes(x=chol, y=slope)) + geom_violin()
c <- ggarrange(cc.3.1, cc.3.2 ,cc.3.3, cc.3.4, cc.3.5, cc.3.6, cc.3.7, cc.3.8 + rremove("x.text"),ncol = 4, nrow = 2)
annotate_figure(c,top = text_grob("chol vs Continuous X variables", color = "black", face = "bold", size = 14))
cc.4.1<- ggplot(heart.dt, aes(x=thalach, y=sex)) + geom_violin()
cc.4.2<- ggplot(heart.dt, aes(x=thalach, y=cp)) + geom_violin()
cc.4.3<- ggplot(heart.dt, aes(x=thalach, y=fbs)) + geom_violin()
cc.4.4<- ggplot(heart.dt, aes(x=thalach, y=restecg)) + geom_violin()
cc.4.5<- ggplot(heart.dt, aes(x=thalach, y=exang)) + geom_violin()
cc.4.6<- ggplot(heart.dt, aes(x=thalach, y=ca)) + geom_violin()
cc.4.7<- ggplot(heart.dt, aes(x=thalach, y=thal)) + geom_violin()
cc.4.8<- ggplot(heart.dt, aes(x=thalach, y=slope)) + geom_violin()
d <- ggarrange(cc.4.1, cc.4.2 ,cc.4.3, cc.4.4, cc.4.5, cc.4.6, cc.4.7, cc.4.8 + rremove("x.text"),ncol = 4, nrow = 2)
annotate_figure(d,top = text_grob("thalach vs Continuous X variables", color = "black", face = "bold", size = 14))
cc.5.1<- ggplot(heart.dt, aes(x=oldpeak, y=sex)) + geom_violin()
cc.5.2<- ggplot(heart.dt, aes(x=oldpeak, y=cp)) + geom_violin()
cc.5.3<- ggplot(heart.dt, aes(x=oldpeak, y=fbs)) + geom_violin()
cc.5.4<- ggplot(heart.dt, aes(x=oldpeak, y=restecg)) + geom_violin()
cc.5.5<- ggplot(heart.dt, aes(x=oldpeak, y=exang)) + geom_violin()
cc.5.6<- ggplot(heart.dt, aes(x=oldpeak, y=ca)) + geom_violin()
cc.5.7<- ggplot(heart.dt, aes(x=oldpeak, y=thal)) + geom_violin()
cc.5.8<- ggplot(heart.dt, aes(x=oldpeak, y=slope)) + geom_violin()
e <- ggarrange(cc.5.1, cc.5.2 ,cc.5.3, cc.5.4, cc.5.5, cc.5.6, cc.5.7, cc.5.8 + rremove("x.text"),ncol = 4, nrow = 2)
annotate_figure(e,top = text_grob("oldpeak vs Continuous X variables", color = "black", face = "bold", size = 14))
#categorical vs categorical
e1<- ggplot(heart.dt, aes(x=cp, y=fbs, col=cp)) + geom_count() + ggtitle("cp vs fbs")
e2<- ggplot(heart.dt, aes(x=restecg, y=exang, col=restecg)) + geom_count() + ggtitle("restecg vs exang")
e3<- ggplot(heart.dt, aes(x=thal, y=fbs, col=thal)) + geom_count() + ggtitle("thal vs fbs")
e4<- ggplot(heart.dt, aes(x=sex, y=restecg, col=sex)) + geom_count() + ggtitle("sex vs restecg")
z <- ggarrange(e1,e2,e3,e4 + rremove("x.text"),ncol = 2, nrow = 2)
annotate_figure(z,top = text_grob("Relationship between categorical X variables", color = "black", face = "bold", size = 14))
## Use correlation map ----
continuous.dt <- heart.dt[,c(1,4,5,8,10)]
corr <- round(cor(continuous.dt),2)
ggcorrplot(corr, hc.order = TRUE, type = "lower", outline.col = "black",
ggtheme = ggplot2::theme_gray,
colors = c("#6D9EC1", "white", "#E46726"),
lab = TRUE)
# Discovered that thal has a category '0'. Removing it because 0 does not represent defect type.
heart.dt[thal=="0",.N]
nrow(heart.dt)
heart.dt<-heart.dt[!thal=="0",]
nrow(heart.dt)
ggplot(heart.dt, aes(thal)) + geom_bar(aes(fill=target)) + ggtitle("Heart disease across defect types")
## Density plot for 1 categorical, 1 numeric ----
# Blue line represents heart disease, red line represents no heart disease.
# Run across all continuous X
ggplot(heart.dt, aes(x=age, col=target)) + geom_density()
ggplot(heart.dt, aes(x=trestbps, col=target)) + geom_density()
ggplot(heart.dt, aes(x=chol, col=target)) + geom_density()
ggplot(heart.dt, aes(x=thalach, col=target)) + geom_density()
ggplot(heart.dt, aes(x=oldpeak, col=target)) + geom_density() #Looks weird
#Categorical X variables
#Target vs Chest Pain Type
z1 = ggplot(data = heart.dt, aes(x = cp, fill = target)) + geom_bar(position = "fill") +
labs(x = "Chest Pain Type", y = "Proportion", title = "Target vs Chest Pain Type") +
geom_text(
aes(label=signif(..count.. / tapply(..count.., ..x.., sum)[as.character(..x..)], digits=3)),
stat="count",
position=position_fill(vjust=0.5), size = 4)
#Target vs Fasting Blood Sugar
z2 = ggplot(data = heart.dt, aes(x = fbs, fill = target)) + geom_bar(position = "fill") +
labs(x = "Presence of Fasting Blood Sugar", y = "Proportion", title = "Target vs Fasting Blood Sugar") +
geom_text(
aes(label=signif(..count.. / tapply(..count.., ..x.., sum)[as.character(..x..)], digits=3)),
stat="count",
position=position_fill(vjust=0.5), size = 4)
#Target vs Resting Electrocardiographic Results
z3 = ggplot(data = heart.dt, aes(x = restecg, fill = target)) + geom_bar(position = "fill") +
labs(x = "Resting Electrocardiographic Results", y = "Proportion", title = "Target vs Resting Electrocardiographic Results") +
geom_text(
aes(label=signif(..count.. / tapply(..count.., ..x.., sum)[as.character(..x..)], digits=3)),
stat="count",
position=position_fill(vjust=0.5), size = 4)
#Target vs Exercise Induced Angina
z4 = ggplot(data = heart.dt, aes(x = exang, fill = target)) + geom_bar(position = "fill") +
labs(x = "Exercise Induced Angina", y = "Proportion", title = "Target vs Exercise Induced Angina") +
geom_text(
aes(label=signif(..count.. / tapply(..count.., ..x.., sum)[as.character(..x..)], digits=3)),
stat="count",
position=position_fill(vjust=0.5), size = 4)
#Target vs Maximum Heart Rate Achieved
z5 = ggplot(data = heart.dt, aes(x = thal, fill = target)) + geom_bar(position = "fill") +
labs(x = "Maximum Heart Rate Achieved", y = "Proportion", title = "Target vs Maximum Heart Rate Achieved") +
geom_text(
aes(label=signif(..count.. / tapply(..count.., ..x.., sum)[as.character(..x..)], digits=3)),
stat="count",
position=position_fill(vjust=0.5), size = 4)
#Target vs Number of Major Blood Vessels
z6 = ggplot(data = heart.dt, aes(x = ca, fill = target)) + geom_bar(position = "fill") +
labs(x = "Number of Major Blood Vessels", y = "Proportion", title = "Target vs Number of Major Blood Vessels") +
geom_text(
aes(label=signif(..count.. / tapply(..count.., ..x.., sum)[as.character(..x..)], digits=3)),
stat="count",
position=position_fill(vjust=0.5), size = 4)
#Target vs Gradient of Slope
z7 = ggplot(data = heart.dt, aes(x = slope, fill = target)) + geom_bar(position = "fill") +
labs(x = "Gradient of Slope", y = "Proportion", title = "Target vs Gradient of Slope") +
geom_text(
aes(label=signif(..count.. / tapply(..count.., ..x.., sum)[as.character(..x..)], digits=3)),
stat="count",
position=position_fill(vjust=0.5), size = 4)
w <- ggarrange(z1, z2 ,z3, z4, z5, z6, z7 + rremove("x.text"),ncol = 2, nrow = 4)
annotate_figure(w,top = text_grob("Relationship between categorical X variables", color = "black", face = "bold", size = 14))
## Data cleaning for logistic regression ----
sum(is.na(heart.dt)) # 0 NA values
skewness(heart.dt$age)
skewness(heart.dt$trestbps)
skewness(heart.dt$chol)
skewness(heart.dt$trestbps)
skewness(heart.dt$oldpeak)
## Building model for prediction ----
set.seed(2004)
split <- sample.split(Y = heart.dt$target, SplitRatio = 0.7)
trainset <- subset(heart.dt, split == T)
testset <- subset(heart.dt, split == F)
heart.log1 <- glm(target~.,data=trainset,family="binomial")
summary(heart.log1) # 467.86
step(heart.log1) ## Backward elimination ----
heart.log2<-glm(formula = target ~ age + sex + cp + trestbps + chol + thalach +
exang + oldpeak + slope + ca + thal, family = "binomial",
data = trainset)
summary(heart.log2) # 464.4
vif(heart.log2)
## Set OR through coefficient level ----
OR <- exp(coef(heart.log2))
OR
## Set OR.CI through confidence interval level ----
OR.CI <- exp(confint(heart.log2))
OR.CI
## MODEL EVALUATION (LOGISTIC REGRESSION) WITH OUTLIERS =========================
## Confusion matrix on TESTSET ----
log.prob <- predict(heart.log2, newdata= testset, type = 'response')
threshold <- 0.5
log.predict.test <- ifelse(log.prob > threshold, "1" , "0" )
log.cm <- table(test.Actual = testset$target, log.predict.test, deparse.level = 2)
log.cm
log.cm.alt <- confusionMatrix(as.factor(log.predict.test), reference = testset$target, positive = "1")
log.cm.alt
## Overall accuracy, sensitivity, specificity on TESTSET ----
## Log1, log1_type1, log1_type2 will be used in a datatable to showcase all accuracies of all models later on
log1 <- round(mean(log.predict.test == testset$target)*100,2) #87.9%
log1_type1 <- (24 / (125 +24 +13 + 144)) * 100
log1_type2 <- (13 / (125 +24 +13 + 144)) * 100
## Confusion matrix on TRAINSET ----
log.prob2 <- predict(heart.log2, type = 'response')
threshold <- 0.5
log.predict.train <- ifelse(log.prob2 > threshold, "1" , "0" )
log.cm2 <- table(train.Actual = trainset$target, log.predict.train, deparse.level = 2)
log.cm2
log.cm2.alt <- confusionMatrix(as.factor(log.predict.train), reference = trainset$target, positive = "1")
log.cm2.alt
## Overall accuracy, sensitivity, specificity on TRAINSET ----
accuracy <- round(mean(log.predict.train == trainset$target)*100,2) #88.9%
accuracy
set.seed(2004)
m1.cart <- rpart(target~., data = trainset , method = 'class', control = rpart.control(minsplit = 15, cp = 0))
print(m1.cart)
printcp(m1.cart)
plotcp(m1.cart)
cverrorcap = m1.cart$cptable[which.min(m1.cart$cptable[,"xerror"]),"xerror"] + m1.cart$cptable[which.min(m1.cart$cptable[,"xerror"]),"xstd"]
i = 1; j=4
while (m1.cart$cptable[i,j] > cverrorcap) {
i = i+1
}
optimal_cp1 = ifelse(i > 1, sqrt(m1.cart$cptable[i,1] * m1.cart$cptable[i-1,1]),1)
m2.cart <- prune(m1.cart, cp = optimal_cp1)
plotcp(m2.cart)
rpart.plot(m2.cart, nn= T, main = "Pruned Tree with new cp")
## Confusion matrix on testset ----
cart.predict.test <- predict(m2.cart, newdata= testset, type = 'class')
result.test <- table(Actual = testset$target, cart.predict.test, deparse.level = 2)
result.test.alt <- confusionMatrix(as.factor(cart.predict.test), reference = testset$target, positive = "1")
result.test.alt
## cart1, cart1_type1,cart1_type2 will be used in a table for analysis of all models
## Overall accuracy, sensitivity, specificity on testset ----
cart1 <- round(mean(cart.predict.test == testset$target)*100,2) #89.22%
cart1 <- mean(testset$target == cart.predict.test)*100
cart1_type1 <- (23 / (126+10+23+147)) * 100
cart1_type2 <- (10 / (126+10+23+147)) * 100
## Confusion matrix on trainset ----
cart.predict.train <- predict(m2.cart, type = 'class')
result.train <- table(Actual = trainset$target, cart.predict.train, deparse.level = 2)
result.train.alt <- confusionMatrix(as.factor(cart.predict.train), reference = trainset$target, positive = "1")
result.train.alt
## Overall accuracy, sensitivity, specificity on trainset ----
accuracy <- round(mean(cart.predict.train == trainset$target)*100,2) #91.71%
## Variable importance ----
m2.cart$variable.importance
## Show variable importance in chart ----
vi1<-
m2.cart$variable.importance %>%
data.frame() %>%
rownames_to_column(var = "Feature") %>%
rename(Overall = '.') %>%
ggplot(aes(x = fct_reorder(Feature, Overall), y = Overall)) +
geom_pointrange(aes(ymin = 0, ymax = Overall), color = "cadetblue", size = .3) +
theme_minimal() +
coord_flip() +
labs(x = "", y = "", title = "Model 1 VI")
vi1
## General overview of present outliers ----
par(mar=c(5,10,2,2), mgp = c(8, 1, 0),las = 1)
data <- heart.dt[,c(1,4:5,8,10)]
boxplot(data,
horizontal = TRUE,names = c(colnames(data)),main = "Boxplot of All Variables (Excluding unwanted X variables and Y variable)",
ylab = "Independent Variables")
## Density plot of each variable to see if data is skewed ----
plot(density(heart.dt$chol)) #Right skewed
plot(density(heart.dt$thalach)) #Left skewed
plot(density(heart.dt$trestbps)) #Right skewed
plot(density(heart.dt$oldpeak)) #Right skewed
## Removing outliers (chol) ----
boxplot(heart.dt$chol, data = heart.dt) # Check for outliers
Q1_chol <- quantile(heart.dt$chol, .25)
Q3_chol <- quantile(heart.dt$chol, .75)
IQR <- IQR(heart.dt$chol)
heart.dt2 <- subset(heart.dt, heart.dt$chol > (Q1_chol -1.5*IQR) & heart.dt$chol < (Q3_chol + 1.5*IQR))
dim(heart.dt2)
# Checking if outliers are removed (chol)
boxplot(heart.dt2$chol, data = heart.dt2)
## Removing outliers (oldpeak) ----
boxplot(heart.dt$oldpeak, data = heart.dt) # Check for outliers
Q1_oldpeak <- quantile(heart.dt$oldpeak, .25)
Q3_oldpeak <- quantile(heart.dt$oldpeak, .75)
IQR_oldpeak <- IQR(heart.dt$oldpeak)
heart.dt2 <- subset(heart.dt, heart.dt$oldpeak > (Q1_oldpeak -1.5*IQR_oldpeak) & heart.dt$oldpeak < (Q3_oldpeak + 1.5*IQR_oldpeak))
dim(heart.dt2)
# Checking if outliers are removed (oldpeak)
boxplot(heart.dt2$oldpeak, data = heart.dt2)
## Removing outliers (thalach) ----
boxplot(heart.dt$thalach, data = heart.dt) # Check for outliers
Q1_thalach <- quantile(heart.dt$thalach, .25)
Q3_thalach <- quantile(heart.dt$thalach, .75)
IQR_thalach <- IQR(heart.dt$thalach)
heart.dt2 <- subset(heart.dt, heart.dt$thalach > (Q1_thalach -1.5*IQR_thalach) & heart.dt$thalach < (Q3_thalach + 1.5*IQR_thalach))
dim(heart.dt2)
# Checking if outliers are removed (thalach)
boxplot(heart.dt2$thalach, data = heart.dt2)
## Removing outliers (trestbps) ----
boxplot(heart.dt$trestbps, data = heart.dt) # Checking for outliers
Q1_trestbps <- quantile(heart.dt$trestbps, .25)
Q3_trestbps <- quantile(heart.dt$trestbps, .75)
IQR_trestbps <- IQR(heart.dt$trestbps)
heart.dt2 <- subset(heart.dt, heart.dt$trestbps > (Q1_trestbps -1.5*IQR_trestbps) & heart.dt$trestbps < (Q3_trestbps + 1.5*IQR_trestbps))
dim(heart.dt2)
# Checking if outliers are removed (trestbps)
boxplot(heart.dt2$trestbps, data = heart.dt2)
## Checking distribution of variables ----
plot(density(heart.dt2$chol)) # Right skewed
plot(density(heart.dt2$thalach)) # Left skewed
plot(density(heart.dt2$trestbps)) # Right skewed
plot(density(heart.dt2$oldpeak)) # Right skewed
set.seed(2004)
split2 <- sample.split(Y = heart.dt2$target, SplitRatio = 0.7)
trainset2 <- subset(heart.dt2, split2 == T)
testset2 <- subset(heart.dt2, split2 == F)
heart.log1_2 <- glm(target~.,data=trainset2,family="binomial")
summary(heart.log1_2) # 431.69
step(heart.log1_2) ## Backward elimination, no variables removed ----
heart.log2_2<-glm(formula = target ~ sex + cp + chol + thalach + oldpeak +
slope + ca + thal, family = "binomial", data = trainset2)
summary(heart.log2_2) # 428.1
## Set OR through coefficient level ----
OR <- exp(coef(heart.log2_2))
OR
## Set OR.CI through confidence interval level ----
OR.CI <- exp(confint(heart.log2_2))
OR.CI
## Confusion matrix on testset ----
log.prob2_2 <- predict(heart.log2_2, newdata= testset2, type = 'response')
threshold <- 0.5
log.predict.test2 <- ifelse(log.prob2_2 > threshold, "1" , "0" )
log.cm_2 <- table(Actual = testset2$target, log.predict.test2, deparse.level = 2)
log.cm_2.alt <- confusionMatrix(as.factor(log.predict.test2), reference = testset2$target, positive = "1")
log.cm_2.alt
## Overall accuracy, sensitivity, specificity on testset ----
accuracy <- round(mean(log.predict.test2 == testset2$target )*100,2) #85.27%, accuracy decreased from 87.9 % , 2.5 % decrease in accuracy.
## Log2, log2_type1, log2_type2 will be used in a table analysis later on
log2 <-(mean(log.predict.test2 == testset2$target ) * 100)
log2_type1 <- (21 / (118 +22+21+131)) * 100
log2_type2 <- (22 / (118 +22+21+131)) * 100
log2
log2
log2
log2
log2
log2
log2
log2
## Confusion matrix on trainset ----
log.prob2_2 <- predict(heart.log2_2, type = 'response')
threshold <- 0.5
log.predict.train2 <- ifelse(log.prob2_2 > threshold, "1" , "0" )
log.cm2_2 <- table(Actual = trainset2$target, log.predict.train2, deparse.level = 2)
log.cm2_2.alt <- confusionMatrix(as.factor(log.predict.train2), reference = trainset2$target, positive = "1")
log.cm2_2.alt
## Overall accuracy, sensitivity, specificity on trainset ----
accuracy <- round(mean(log.predict.train2 == trainset2$target)*100,2) #88.69%, 3% higher than testset
set.seed(2004)
m1.cart2 <- rpart(target~., data = trainset2 , method = 'class', control = rpart.control(minsplit = 15, cp = 0))
print(m1.cart2)
printcp(m1.cart2)
plotcp(m1.cart2)
cverrorcap = m1.cart2$cptable[which.min(m1.cart2$cptable[,"xerror"]),"xerror"] + m1.cart2$cptable[which.min(m1.cart2$cptable[,"xerror"]),"xstd"]
i = 1; j=4
while (m1.cart2$cptable[i,j] > cverrorcap) {
i = i+1
}
optimal_cp2 = ifelse(i > 1, sqrt(m1.cart2$cptable[i,1] * m1.cart2$cptable[i-1,1]),1)
m2.cart2 <- prune(m1.cart2, cp = optimal_cp2)
plotcp(m2.cart2)
rpart.plot(m2.cart2, nn= T, main = "Pruned Tree with new cp")
## Overall accuracy, sensitivity, specificity on trainset ----
cart.predict.train2 <- predict(m2.cart2, type = 'class')
result.train2 <- table(Actual = trainset2$target, cart.predict.train2, deparse.level = 2)
mean(trainset2$target == cart.predict.train2) #88.8% , increased in accuracy as compared to model with outliers.
## Overall accuracy, sensitivity, specificity on testset ----
cart.predict.test2 <- predict(m2.cart2, newdata= testset2, type = 'class')
result.test2 <- table(Actual = testset2$target, cart.predict.test2, deparse.level = 2)
mean(testset2$target == cart.predict.test2) #85.61%
## cart2, cart2_type1, cart2_type2 will be used in a data table for analysis later
cart2 = mean(testset2$target == cart.predict.test2)  * 100
cart2_type1 = (20 / (119+20+22+131)) * 100
cart2_type2 = (22 / (119+20+22+131)) * 100
## Variable importance in cart2 model ----
m2.cart2$variable.importance
## Show variable importance in chart ----
vi2<-
m2.cart2$variable.importance %>%
data.frame() %>%
rownames_to_column(var = "Feature") %>%
rename(Overall = '.') %>%
ggplot(aes(x = fct_reorder(Feature, Overall), y = Overall)) +
geom_pointrange(aes(ymin = 0, ymax = Overall), color = "cadetblue", size = .3) +
theme_minimal() +
coord_flip() +
labs(x = "", y = "", title = "Model 2 VI")
vi2
set.seed(2004)
summary(trainset2$target)
balancetrain_data = ovun.sample(target~., data=trainset2, method = "over", N = (2*357))$data
table(balancetrain_data$target)
summary(balancetrain_data)
heart.log1_3 <- glm(target~.,data=balancetrain_data,family="binomial")
summary(heart.log1_3)# 455.45
step(heart.log1_3) # Backward elimination, no variables removed ----
heart.log2_3<- glm(formula = target ~ age + sex + cp + trestbps + chol + restecg +
thalach + oldpeak + slope + ca + thal, family = "binomial",
data = balancetrain_data)
summary(heart.log2_3) # 452.7
## Set OR through coefficient level ----
OR <- exp(coef(heart.log2_3))
OR
## Set OR.CI through confidence interval level ----
OR.CI <- exp(confint(heart.log2_3))
OR.CI
## Confusion matrix on testset ----
log.prob2_3 <- predict(heart.log2_3, newdata= testset2, type = 'response')
threshold <- 0.5
log.predict.test3 <- ifelse(log.prob2_3 > threshold, "0" , "1" )
log.cm_3 <- table(Actual = testset2$target, log.predict.test3, deparse.level = 2)
log.cm_3
## Overall accuracy on testset ----
mean(log.predict.test3 == testset2$target ) # 85.95%
## Log3, log3_type1, log3_type2 will be used in analysis later
log3 = mean(log.predict.test3 == testset2$target )*100
log3_type1 = (21 / (118 +21 +20 +133)) * 100
log3_type2 = (20 / (118 +21 +20 +133)) * 100
log.cm_3
log.cm_3
log.cm_3
## Overall accuracy on testset ----
mean(log.predict.test3 == testset2$target ) # 85.95%
## Log3, log3_type1, log3_type2 will be used in analysis later
log3 = mean(log.predict.test3 == testset2$target )*100
log3_type1 = (21 / (118 +21 +20 +133)) * 100
log3_type2 = (20 / (118 +21 +20 +133)) * 100
## Confusion matrix on trainset ----
log.prob2_3 <- predict(heart.log2_3, type = 'response')
threshold <- 0.5
log.predict.train3 <- ifelse(log.prob2_3> threshold, "0" , "1" )
log.cm2_3 <- table(Actual = balancetrain_data$target, log.predict.train3, deparse.level = 2)
log.cm2_3
## Overall accuracy on trainset ----
mean(log.predict.train3 == balancetrain_data$target) #89.5%, very close to testset accuracy.
set.seed(2004)
m1.cart3 <- rpart(target~., data = balancetrain_data , method = 'class', control = rpart.control(minsplit = 15, cp = 0))
print(m1.cart3)
printcp(m1.cart3)
plotcp(m1.cart3)
cverrorcap = m1.cart3$cptable[which.min(m1.cart3$cptable[,"xerror"]),"xerror"] + m1.cart3$cptable[which.min(m1.cart3$cptable[,"xerror"]),"xstd"]
i = 1; j=4
while (m1.cart3$cptable[i,j] > cverrorcap) {
i = i+1
}
optimal_cp3 = ifelse(i > 1, sqrt(m1.cart3$cptable[i,1] * m1.cart3$cptable[i-1,1]),1)
m2.cart3 <- prune(m1.cart3, cp = optimal_cp3)
plotcp(m2.cart3)
rpart.plot(m2.cart3, nn= T, main = "Pruned Tree with new cp")
## Accuracy on test set ----
cart.predict.test3 <- predict(m2.cart3, newdata= testset2, type = 'class')
result.test3 <- table(Actual = testset2$target, cart.predict.test3, deparse.level = 2)
mean(testset2$target == cart.predict.test3) # 90%, accuracy increases
## Cart3, Cart3_type1, Cart3_type2 will be used later on for analysis in a data table
cart3 = mean(testset2$target == cart.predict.test3) * 100
cart3_type1 = (16 / (16+123+140+13)) *100
cart3_type2 = (13 / (16+123+140+13)) *100
## Accuracy on trainset ----
cart.predict.train3 <- predict(m2.cart3, type = 'class')
result.train3 <- table(Actual = balancetrain_data$target, cart.predict.train3, deparse.level = 2)
mean(balancetrain_data$target == cart.predict.train3) # 94.81%
## Variable importance in cart2 model ----
m2.cart3$variable.importance
## Show variable importance in chart ----
vi3<-
m2.cart3$variable.importance %>%
data.frame() %>%
rownames_to_column(var = "Feature") %>%
rename(Overall = '.') %>%
ggplot(aes(x = fct_reorder(Feature, Overall), y = Overall)) +
geom_pointrange(aes(ymin = 0, ymax = Overall), color = "cadetblue", size = .3) +
theme_minimal() +
coord_flip() +
labs(x = "", y = "", title = "Model 3 VI")
vi3
#Putting all the cart trees together
par(mfrow=c(3,1))
rpart.plot(m2.cart, nn= T, main = "Pruned Tree cart model 1")
rpart.plot(m2.cart2, nn= T, main = "Pruned Tree cart model 2")
rpart.plot(m2.cart3, nn= T, main = "Pruned Tree cart model 3")
par(mfrow=c(1,1))
#Put variable importance plot for all three models together.
ggarrange(vi1, vi2, vi3 + rremove("x.text"),
ncol = 3, nrow = 1)
accuracy_table = data.frame("Accuracy of Different Models" = 6:1)
rownames(accuracy_table) = c("Log1", "Log2", "Log3", "CART 1" , "CART 2", "CART 3")
accuracy_table[1,1] = log1
accuracy_table[1,2] = log1_type1
accuracy_table[1,3] = log1_type2
accuracy_table[2,1] = log2
accuracy_table[2,2] = log2_type1
accuracy_table[2,3] = log2_type2
accuracy_table[3,1] = log3
accuracy_table[3,2] = log3_type1
accuracy_table[3,3] = log3_type2
accuracy_table[4,1] = cart1
accuracy_table[4,2] = cart1_type1
accuracy_table[4,3] = cart1_type2
accuracy_table[5,1] = cart2
accuracy_table[5,2] = cart2_type1
accuracy_table[5,3] = cart2_type2
accuracy_table[6,1] = cart3
accuracy_table[6,2] = cart3_type1
accuracy_table[6,3] = cart3_type2
colnames(accuracy_table) = c("Model Accuracy In Percentage", "TYPE 1 ERROR RATE", "TYPE 2 ERROR RATE")
accuracy_table
## General overview of present outliers ----
par(mar=c(5,10,2,2), mgp = c(8, 1, 0),las = 1)
data <- heart.dt[,c(1,4:5,8,10)]
boxplot(data,
horizontal = TRUE,names = c(colnames(data)),main = "Boxplot of All Variables (Excluding unwanted X variables and Y variable)",
ylab = "Independent Variables")
## Data cleaning for logistic regression ----
sum(is.na(heart.dt)) # 0 NA values
